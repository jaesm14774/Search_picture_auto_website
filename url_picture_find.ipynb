{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "user_agent = [\n",
    "\"Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0\",\n",
    "\"Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko\",\n",
    "\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\",\n",
    "\"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11\",\n",
    "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)\",\n",
    "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\"\n",
    "]\n",
    "\n",
    "\n",
    "def GET_PICTURE(url):\n",
    "    b=requests.get(url,headers={'User-Agent':np.random.choice(user_agent)})\n",
    "    #find header of url\n",
    "    if bool(re.search(string=b.url,pattern='http')):\n",
    "        addition_head=re.search(string=b.url,pattern='(http(s)*://[a-zA-Z0-9\\.]+)/').group(1)\n",
    "    else:\n",
    "        addition_head=''\n",
    "    b.encoding='utf-8'\n",
    "    soup=BeautifulSoup(b.text,'lxml')\n",
    "    #delete recommend nodes\n",
    "    try:\n",
    "        for s in soup.find_all(class_=re.compile('.*recommend.*|.*more.*|[^a-zA-Z]? ?related ?[^a-zA-Z ]+|[^a-zA-Z]+ ?related ?[^a-zA-Z ]?|^related$|.*aside.*')):\n",
    "            s.decompose()\n",
    "    except:\n",
    "        soup=soup\n",
    "    Result=[]\n",
    "    #remove some annoyous fake image\n",
    "    bad_words=['template','icon','logo','gif','/ad/']\n",
    "    bad_pat=''\n",
    "    for i,s in enumerate(bad_words):\n",
    "        if i == 0:\n",
    "            bad_pat=bad_pat+s\n",
    "        else:\n",
    "            bad_pat=bad_pat+'|'+s\n",
    "    #find correct position of article\n",
    "    N=0\n",
    "    try:\n",
    "        if soup.find('h1').text != '':\n",
    "            article_body=soup.find('h1').find_parent()\n",
    "        else:\n",
    "            article_body=soup.find('h2').find_parent()\n",
    "    except:\n",
    "        try:\n",
    "            article_body=soup.find('h2').find_parent()\n",
    "        except:\n",
    "            try:\n",
    "                article_body=soup.find(class_=re.compile('.*title.*')).find_parent()\n",
    "            except:\n",
    "                N=100\n",
    "                article_body='QQ,not exist h1 h2 anymore'\n",
    "    #try to find correct article body\n",
    "    try:\n",
    "        while len(str(article_body))<5000 and N <20:\n",
    "            article_body=article_body.find_parent()\n",
    "            N=N+1\n",
    "        if N == 100:\n",
    "            raise RuntimeError('QQ,skip')\n",
    "        article_body=article_body.find_all('img')\n",
    "    except:\n",
    "        try:\n",
    "            article_body=soup.find_all('img')\n",
    "        except:\n",
    "            return('No pictures or can not deal with.')\n",
    "    #find correct type of file of picture\n",
    "    for s in article_body:\n",
    "        try:\n",
    "            node_name=re.search(string=str(s),pattern='\\s*([-_a-zA-Z]*src)=').group(1)\n",
    "            if bool(re.search(string=s.get(node_name),pattern='(jpg|jpeg|png)')) and not bool(re.search(string=s.get(node_name),pattern=bad_pat)):\n",
    "                Result.append(s.get(node_name))\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "    if len(Result) == 0:\n",
    "        for s in article_body:\n",
    "            try:\n",
    "                node_name=re.search(string=str(s),pattern='\\s*([-_a-zA-Z]*src)=').group(1)\n",
    "                if not bool(re.search(string=s.get(node_name),pattern=bad_pat)):\n",
    "                    Result.append(s.get(node_name))\n",
    "                else:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "    #try to return original url of picture\n",
    "    if len(Result)>0:\n",
    "        temp=[bool(re.search(string=s,pattern='^http')) for s in Result]\n",
    "        if False in temp and True in temp:\n",
    "            Result=Result\n",
    "#             Result=[s for i,s in enumerate(Result) if not temp[i]]\n",
    "        elif False not in temp and True in temp:\n",
    "            return Result\n",
    "        else:\n",
    "            Result=Result\n",
    "        #../../../n12377419/n12378191/c15390423/part/15390438.jpg\n",
    "        #clean url in result\n",
    "        not_complete=[]   #省略主要網址的關鍵網站\n",
    "        seem_complete=[]  #省略了http or https 的網址\n",
    "        for i,s in enumerate(Result):\n",
    "            if bool(re.search(string=s,pattern='^//')):\n",
    "                seem_complete.append(i)\n",
    "                continue\n",
    "            if bool(re.search(string=s,pattern='^http')):\n",
    "                Result[i]=Result[i]\n",
    "            else:\n",
    "                s=re.sub(string=Result[i],pattern='\\.\\./',repl='')\n",
    "                if not bool(re.search(string=s,pattern='^/')):\n",
    "                    s='/'+s\n",
    "                Result[i]=s\n",
    "                not_complete.append(i)\n",
    "        #處理省略了http or https 的網址\n",
    "        if len(seem_complete)>0:\n",
    "            for ii in seem_complete:\n",
    "                if requests.get('https:'+Result[ii]).status_code != 200:\n",
    "                    Result[ii]='http:'+Result[ii]\n",
    "                else:\n",
    "                    Result[ii]='https:'+Result[ii]\n",
    "        #find correct addition_header \n",
    "        #prevent infinite loop\n",
    "        if len(not_complete)>0:\n",
    "            N=0\n",
    "            while requests.get(addition_head+Result[not_complete[0]]).status_code != 200:\n",
    "                if N == 10:\n",
    "                    break\n",
    "                addition_head=addition_head+'/'+re.sub(string=b.url,pattern=addition_head+'/',repl='').split('/')[0]\n",
    "                N=N+1\n",
    "            for s in not_complete:\n",
    "                Result[s]=addition_head+Result[s]\n",
    "        return Result\n",
    "    else:\n",
    "        return('No pictures or can not deal with.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_PICTURE('https://news.ltn.com.tw/news/life/breakingnews/3087051')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.post('http://103.124.74.68:3266/GET_PICTURE',\n",
    "              data={'url':'https://news.ltn.com.tw/news/life/breakingnews/3087051'}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_PICTURE('https://cn.reuters.com/article/japan-nov-industry-retail-sliding-1227-idCNKBS1YV061')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_PICTURE('https://udn.com/news/story/120940/4404291')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_PICTURE('https://www.ainonline.com/aviation-news/air-transport/2020-03-19/us-seeks-strike-balance-industry-pleads-bailouts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
